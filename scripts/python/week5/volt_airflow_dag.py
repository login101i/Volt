#!/usr/bin/env python3
"""
PrzykÅ‚ad DAG Airflow dla aplikacji Volt
Data Engineering Roadmap - TydzieÅ„ 2, Dni 8-9: Airflow + S3 + Volt
"""
from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from airflow.operators.bash import BashOperator
from airflow.utils.trigger_rule import TriggerRule
from airflow.sensors.filesystem import FileSensor
from datetime import datetime, timedelta
import os
import sys

# Dodaj Å›cieÅ¼kÄ™ do skryptÃ³w Volt
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from week4.volt_data_to_s3 import export_components_to_json, upload_components_to_s3
from week4.volt_images_to_s3 import migrate_images_to_s3
from week4.presigned_urls import batch_generate_presigned_urls

# Default arguments dla DAG
default_args = {
    'owner': 'volt_data_team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=2)
}

# GÅ‚Ã³wny DAG dla aplikacji Volt
dag = DAG(
    'volt_daily_pipeline',
    default_args=default_args,
    description='Codzienny pipeline danych dla aplikacji Volt: PostgreSQL â†’ S3 â†’ Presigned URLs',
    schedule_interval='@daily',  # Codziennie o pÃ³Å‚nocy
    catchup=False,  # Nie przetwarzaj historycznych dat
    max_active_runs=1,  # Tylko jedno uruchomienie na raz
    tags=['volt', 'data-engineering', 's3', 'postgresql']
)

def check_volt_database():
    """SprawdÅº poÅ‚Ä…czenie z bazÄ… danych Volt"""
    try:
        from week4.volt_data_to_s3 import get_postgres_connection
        conn = get_postgres_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM electrical_components")
        count = cursor.fetchone()[0]
        conn.close()
        print(f"âœ… Baza danych Volt dostÄ™pna: {count} komponentÃ³w")
        return 'extract_volt_data'
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d poÅ‚Ä…czenia z bazÄ… Volt: {e}")
        return 'alert_failure'

def extract_volt_data():
    """Pobierz dane komponentÃ³w z PostgreSQL"""
    print("ğŸ“Š Rozpoczynam ekstrakcjÄ™ danych z aplikacji Volt...")

    try:
        components = export_components_to_json()

        if not components:
            raise ValueError("Brak danych komponentÃ³w do przetworzenia")

        # Zapisz do pliku tymczasowego (Airflow moÅ¼e tego uÅ¼yÄ‡)
        temp_file = '/tmp/volt_components_raw.json'
        import json
        with open(temp_file, 'w', encoding='utf-8') as f:
            json.dump(components, f, indent=2, ensure_ascii=False)

        print(f"âœ… Wyekstrahowano {len(components)} komponentÃ³w do {temp_file}")
        return temp_file

    except Exception as e:
        print(f"âŒ BÅ‚Ä…d ekstrakcji danych: {e}")
        raise

def transform_volt_data():
    """PrzeksztaÅ‚Ä‡ dane komponentÃ³w uÅ¼ywajÄ…c Pandas"""
    print("ğŸ”„ Rozpoczynam transformacjÄ™ danych Volt...")

    try:
        import pandas as pd

        # Wczytaj dane z pliku tymczasowego
        temp_file = '/tmp/volt_components_raw.json'
        df = pd.read_json(temp_file)

        # Transformacje danych
        # 1. UsuÅ„ komponenty bez ceny
        df_clean = df.dropna(subset=['price'])

        # 2. Filtruj komponenty z prawidÅ‚owym zakresem cen
        df_clean = df_clean[(df_clean['price'] > 0) & (df_clean['price'] < 10000)]

        # 3. Dodaj kolumnÄ™ kategorii cenowej
        df_clean['price_category'] = pd.cut(
            df_clean['price'],
            bins=[0, 100, 500, 1000, float('inf')],
            labels=['budget', 'standard', 'premium', 'luxury']
        )

        # 4. Normalizuj nazwy (lowercase, usuÅ„ specjalne znaki)
        df_clean['name_normalized'] = df_clean['name'].str.lower().str.replace(r'[^\w\s]', '', regex=True)

        # Zapisz przeksztaÅ‚cone dane
        transformed_file = '/tmp/volt_components_transformed.parquet'
        df_clean.to_parquet(transformed_file, index=False)

        print(f"âœ… PrzeksztaÅ‚cono {len(df)} â†’ {len(df_clean)} komponentÃ³w")
        print(f"   Zapisano do: {transformed_file}")

        return transformed_file

    except Exception as e:
        print(f"âŒ BÅ‚Ä…d transformacji danych: {e}")
        raise

def load_to_s3():
    """ZaÅ‚aduj dane do S3"""
    print("ğŸ“¤ Rozpoczynam Å‚adowanie danych do S3...")

    try:
        # Upload przeksztaÅ‚conych danych
        transformed_file = '/tmp/volt_components_transformed.parquet'

        # UÅ¼yj istniejÄ…cej funkcji upload
        from week4.volt_data_to_s3 import upload_components_to_s3
        import pandas as pd

        # Wczytaj i przeÅ›lij jako JSON (dla kompatybilnoÅ›ci)
        df = pd.read_parquet(transformed_file)
        components_data = df.to_dict('records')

        s3_key = upload_components_to_s3(components_data)

        if s3_key:
            print(f"âœ… Dane zaÅ‚adowane do S3: {s3_key}")
            return s3_key
        else:
            raise ValueError("Nie udaÅ‚o siÄ™ przesÅ‚aÄ‡ danych do S3")

    except Exception as e:
        print(f"âŒ BÅ‚Ä…d Å‚adowania do S3: {e}")
        raise

def generate_image_urls():
    """Wygeneruj presigned URLs dla zdjÄ™Ä‡ komponentÃ³w"""
    print("ğŸ”— Generowanie presigned URLs dla zdjÄ™Ä‡ komponentÃ³w...")

    try:
        # Pobierz wszystkie ID komponentÃ³w z bazy
        from week4.volt_data_to_s3 import get_postgres_connection

        conn = get_postgres_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT DISTINCT id FROM electrical_components")
        component_ids = [str(row[0]) for row in cursor.fetchall()]
        conn.close()

        if not component_ids:
            print("âš ï¸  Brak komponentÃ³w w bazie danych")
            return []

        # Wygeneruj URLs
        results = batch_generate_presigned_urls(component_ids, expiration_hours=168)  # 7 dni

        successful_urls = len(results.get('successful', []))
        failed_urls = len(results.get('failed', []))

        print(f"âœ… Wygenerowano {successful_urls} presigned URLs")
        if failed_urls > 0:
            print(f"âš ï¸  {failed_urls} URLs nie udaÅ‚o siÄ™ wygenerowaÄ‡")

        return results

    except Exception as e:
        print(f"âŒ BÅ‚Ä…d generowania presigned URLs: {e}")
        raise

def cleanup_temp_files():
    """WyczyÅ›Ä‡ pliki tymczasowe"""
    import os

    temp_files = [
        '/tmp/volt_components_raw.json',
        '/tmp/volt_components_transformed.parquet'
    ]

    for file_path in temp_files:
        try:
            if os.path.exists(file_path):
                os.remove(file_path)
                print(f"ğŸ—‘ï¸  UsuniÄ™to plik tymczasowy: {file_path}")
        except Exception as e:
            print(f"âš ï¸  Nie udaÅ‚o siÄ™ usunÄ…Ä‡ {file_path}: {e}")

def send_success_alert():
    """WyÅ›lij alert o sukcesie"""
    print("ğŸ‰ Pipeline Volt zakoÅ„czony pomyÅ›lnie!")
    print("ğŸ“Š Podsumowanie:")
    print("   âœ… Ekstrakcja danych z PostgreSQL")
    print("   âœ… Transformacja Pandas")
    print("   âœ… Åadowanie do S3")
    print("   âœ… Generowanie presigned URLs")
    print("   ğŸ§¹ Czyszczenie plikÃ³w tymczasowych")

def send_failure_alert():
    """WyÅ›lij alert o bÅ‚Ä™dzie"""
    print("âŒ Pipeline Volt zakoÅ„czony bÅ‚Ä™dem!")
    print("ğŸ” SprawdÅº logi Airflow dla szczegÃ³Å‚Ã³w")

# Definicja zadaÅ„ (tasks)

# 1. SprawdÅº poÅ‚Ä…czenie z bazÄ… danych
check_db = BranchPythonOperator(
    task_id='check_volt_database',
    python_callable=check_volt_database,
    dag=dag
)

# 2. Ekstrakcja danych
extract_data = PythonOperator(
    task_id='extract_volt_data',
    python_callable=extract_volt_data,
    dag=dag
)

# 3. Transformacja danych
transform_data = PythonOperator(
    task_id='transform_volt_data',
    python_callable=transform_volt_data,
    dag=dag
)

# 4. Åadowanie do S3
load_s3 = PythonOperator(
    task_id='load_to_s3',
    python_callable=load_to_s3,
    dag=dag
)

# 5. Generowanie presigned URLs
generate_urls = PythonOperator(
    task_id='generate_image_urls',
    python_callable=generate_image_urls,
    dag=dag
)

# 6. Czyszczenie plikÃ³w tymczasowych
cleanup = PythonOperator(
    task_id='cleanup_temp_files',
    python_callable=cleanup_temp_files,
    trigger_rule=TriggerRule.ALL_DONE,  # Uruchom zawsze, nawet przy bÅ‚Ä™dach
    dag=dag
)

# 7. Alerty
success_alert = PythonOperator(
    task_id='success_alert',
    python_callable=send_success_alert,
    trigger_rule=TriggerRule.ALL_SUCCESS,
    dag=dag
)

failure_alert = PythonOperator(
    task_id='failure_alert',
    python_callable=send_failure_alert,
    trigger_rule=TriggerRule.ONE_FAILED,
    dag=dag
)

# PrzepÅ‚yw zadaÅ„ (dependencies)
check_db >> [extract_data, failure_alert]
extract_data >> transform_data >> load_s3 >> generate_urls >> cleanup
cleanup >> [success_alert, failure_alert]

if __name__ == "__main__":
    print("ğŸš€ Test DAG Volt - uruchomienie lokalne")

    # Test pojedynczych funkcji
    try:
        print("\n1. Sprawdzanie bazy danych...")
        check_result = check_volt_database()

        if check_result == 'extract_volt_data':
            print("\n2. Ekstrakcja danych...")
            extract_volt_data()

            print("\n3. Transformacja danych...")
            transform_volt_data()

            print("\n4. Åadowanie do S3...")
            load_to_s3()

            print("\n5. Generowanie URLs...")
            generate_image_urls()

            print("\n6. Czyszczenie...")
            cleanup_temp_files()

            print("\nğŸ‰ Test DAG zakoÅ„czony pomyÅ›lnie!")
        else:
            print(f"âŒ Test nieudany: {check_result}")

    except Exception as e:
        print(f"âŒ BÅ‚Ä…d podczas testu DAG: {e}")
        import traceback
        traceback.print_exc()