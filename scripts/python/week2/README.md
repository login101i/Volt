# Week 2: SQL & Model Danych

Ten folder zawiera zadania z **Tygodnia 2** roadmapy Data Engineer w projekcie Volt.

## Zawarto≈õƒá folderu

### Pliki g≈Ç√≥wne:
- `postgres_crud.py` - Podstawowe operacje CRUD na PostgreSQL
- `create_volt_schema.py` - Tworzenie schematu bazy danych dla aplikacji Volt
- `migrate_volt_data.py` - Migracja danych miƒôdzy systemami

## Zadania z Tygodnia 2

Na podstawie [data-engineer-readme.md](../../frontend/app/interview/data-engineer-readme.md):

### ‚úÖ Zaimplementowane:
- **Postgres / Redshift basics** - `postgres_crud.py`
  - Operacje CREATE, INSERT, SELECT, UPDATE, DELETE
  - Po≈ÇƒÖczenie z bazƒÖ PostgreSQL na AWS RDS
  - Parametryzowane zapytania dla bezpiecze≈Ñstwa

### üîÑ Do zaimplementowania:
- **CTE (Common Table Expressions)** - Zapytania z `WITH`
- **Window functions** - Funkcje okna (ranking, sumy ruchome)
- **Indexing** - Optymalizacja wydajno≈õci zapyta≈Ñ
- **Schema bazy: raw tables, staging, marts** - Architektura warstwowa danych

## Architektura Warstwowa Danych

### Raw Layer (surowe dane)
- Pe≈Çne kopie danych z systemu ≈∫r√≥d≈Çowego
- Audyt, odzyskiwanie, powtarzalno≈õƒá ETL
- Format: JSON, CSV, logi

### Staging Layer (oczyszczone dane)
- Dane po transformacji i walidacji
- Casty typ√≥w, standaryzacja, brak nulli
- Przygotowane do dalszego przetwarzania

### Marts Layer (dane analityczne)
- Dane gotowe do raportowania i analizy
- Agregacje, denormalizacja, obliczone kolumny
- Optymalizacja pod wydajno≈õƒá zapyta≈Ñ

## Jak u≈ºywaƒá

### 1. Konfiguracja ≈õrodowiska
```bash
# Zainstaluj zale≈ºno≈õci
pip install -r ../requirements.txt

# Skonfiguruj plik .env z danymi po≈ÇƒÖczenia do PostgreSQL
cp .env.example .env
# Edytuj .env z w≈Ça≈õciwymi danymi RDS
```

### 2. Uruchomienie operacji CRUD
```bash
python postgres_crud.py
```

### 3. Tworzenie schematu bazy
```bash
python create_volt_schema.py
```

### 4. Migracja danych
```bash
python migrate_volt_data.py
```

## Wymagania

- Python 3.8+
- PostgreSQL (AWS RDS lub lokalny)
- Zainstalowane biblioteki z `requirements.txt`
- Skonfigurowane po≈ÇƒÖczenie z bazƒÖ danych w pliku `.env`

## Bezpiecze≈Ñstwo

- U≈ºywaj pliku `.env` dla danych po≈ÇƒÖczenia (nie commituj do git!)
- Parametryzowane zapytania zapobiegajƒÖ SQL injection
- Dane wra≈ºliwe sƒÖ chronione zmiennymi ≈õrodowiskowymi

## Nastƒôpne kroki

Po uko≈Ñczeniu Tygodnia 2 przejd≈∫ do:
- **Tydzie≈Ñ 3**: S3 jako Data Lake (folder `../week3/`)
- Implementacja ETL pipeline'√≥w
- Integracja z AWS us≈Çugami