# Data Engineer Roadmap Volt ‚Äì praktyka na podstawie roadmapy (Node.js, Next.js API, Python)

## Cel projektu
Volt ewoluuje w realny projekt data engineeringowy ‚Äì nie tylko teoria, ale wdro≈ºenia hands-on! Stack: Node.js/Next.js API + Python + AWS.

---

## Tydzie≈Ñ 1: AWS & Architektura
Cel: rozumiesz, co gdzie stoi i dlaczego 

**Checklist:**
- [x] **VPC:** Izoluj zasoby AWS w≈ÇasnƒÖ, prywatnƒÖ sieciƒÖ (stw√≥rz VPC przez konsolƒô, Terraform lub AWS CLI). 
- [x] **public / private subnet:** Utw√≥rz subnety i po≈ÇƒÖcz je z VPC ‚Äì backend w prywatnych, LB/API w publicznych (high-availability!).
- [x] **security groups:** Ustaw regu≈Çy dostƒôpu: tylko niezbƒôdne porty, backend/API ‚Äì tylko swoje grupy/security references! 
- [x] **EC2 public + private:** Postaw EC2 (public jako bastion, prywatne pod backend/Python/ETL). 
- [x] **RDS (Postgres):** Postaw bazƒô (tylko w prywatnym subnetcie!) i po≈ÇƒÖcz z Next.js API lub Python workerem, nie otwieraj do ≈õwiata. 
- [x] **Route53:** Obs≈Çu≈º routing domen (np. pod API, LB, backend, dev/prod ≈õrodowiska). Testuj r√≥≈ºne typy rekord√≥w. 
- [x] **Reverse proxy (Caddy/Nginx):** ZarzƒÖdzaj SSL, rozdziel ruchem do us≈Çug (np. /api ‚Üí backend, /airflow ‚Üí Airflow UI) na jednej maszynie/instancji. 

**Output:**
- diagram architektury (np. draw.io)
- README czemu tak (decyzje architektoniczne)

---

## Tydzie≈Ñ 2: SQL & Model Danych
Cel: my≈õlisz jak Data Engineer, nie jak "SELECT *"

**Checklist:**
- [x] **Postgres / Redshift basics:** Postaw bazƒô, skonfiguruj po≈ÇƒÖczenie z aplikacjƒÖ, wykonaj inspekcjƒô bazowych operacji CRUD przez Node/Python.
  - ‚úÖ Utworzono skrypt Python: `scripts/python/postgres_crud.py`
  - ‚úÖ Zobacz instrukcje poni≈ºej w sekcji "Postgres / Redshift basics - Implementacja"
- [ ] **CTE:** Napisaƒá kilka zapyta≈Ñ z CTE (`WITH`), oswoiƒá siƒô z refaktoryzacjƒÖ zagnie≈ºd≈ºonych SELECT√≥w.
- [ ] **window functions:** Zr√≥b query z oknem (np. ranking, sumy ruchome) ‚Äì przydatne w raportach!
- [ ] **indexing (teoria + praktyka):** Dodaj indeksy do tabel (po kluczach), sprawd≈∫ wydajno≈õƒá.
- [ ] **schema bazy: raw tables, staging, marts:** Zaprojektuj model warstwowy (surowe/staging/analityczne tabele); mo≈ºesz podzieliƒá schemat na pliki SQL/dbt.

## üéØ Podsumowanie: Architektura Warstwowa Danych (Raw ‚Üí Staging ‚Üí Marts)

### Dlaczego w og√≥le tworzymy warstwy?

**Cel: podzia≈Ç danych na r√≥≈ºne poziomy "dojrza≈Ço≈õci" i kontroli jako≈õci.**

- **Raw (surowe)** ‚Äì pe≈Çne kopie danych z systemu ≈∫r√≥d≈Çowego, bez zmian.
  - Audyt, odzyskiwanie, powtarzalno≈õƒá ETL.
  - Mo≈ºna trzymaƒá w dowolnym formacie (JSONB, CSV, logi).

- **Staging (przygotowane)** ‚Äì dane ju≈º oczyszczone i ustrukturyzowane.
  - Casty typ√≥w, brak nulli w kluczowych kolumnach, standaryzacja.
  - Nadal blisko ≈∫r√≥d≈Ça, ale ju≈º mo≈ºna robiƒá transformacje.

- **Msarts (analityczne / raportowe)** ‚Äì dane gotowe do raportowania i analizy.
  - Agregacje, denormalizacja, obliczone kolumny (np. total_orders w naszym users_mart).
  - Tu zak≈Çada siƒô wysokƒÖ wydajno≈õƒá zapyta≈Ñ ‚Äì indeksy, materializacje, partycje.

**PodsumowujƒÖc:** ka≈ºda warstwa daje kontrolƒô nad transformacjƒÖ danych i u≈Çatwia debugowanie: je≈õli raport jest b≈Çƒôdny, mo≈ºesz sprawdziƒá raw ‚Üí staging ‚Üí mart krok po kroku.

### 2Ô∏è‚É£ Co mo≈ºemy robiƒá na ka≈ºdej warstwie

| Warstwa | Co robimy | Przyk≈Çady |
|---------|-----------|-----------|
| **Raw** | Archiwizacja, audyt | Trzymanie JSON√≥w z API, pe≈Çnych dump√≥w, log√≥w. |
| **Staging** | Czyszczenie, typy, walidacja | CAST JSON ‚Üí kolumny, usuniƒôcie duplikat√≥w, normalizacja nazw. |
| **Marts** | Analiza, raporty, agregacje | Liczenie sum, ranking u≈ºytkownik√≥w, ostatnie zam√≥wienie, liczba zam√≥wie≈Ñ, widoki do BI. |

### 3Ô∏è‚É£ Korzy≈õci z takiego podej≈õcia

- **Bezpiecze≈Ñstwo i audyt** ‚Äì surowe dane pozostajƒÖ nienaruszone.
- **≈Åatwo≈õƒá debugowania** ‚Äì je≈õli co≈õ jest ≈∫le w raporcie, sprawdzasz krok po kroku: raw ‚Üí staging ‚Üí mart.
- **Skalowalno≈õƒá** ‚Äì w miarƒô rozrostu danych, mo≈ºesz przetwarzaƒá tylko staging/marts zamiast od nowa ca≈Çych danych.
- **Przygotowanie pod ETL / ELT / BI** ‚Äì warstwy idealnie wsp√≥≈ÇgrajƒÖ z narzƒôdziami typu dbt, Airflow, Looker, Power BI.
- **Wydajno≈õƒá** ‚Äì w marts mo≈ºesz dodawaƒá indeksy, partycje, materializowane widoki, ≈ºeby raporty dzia≈Ça≈Çy szybko.

### 4Ô∏è‚É£ Przyk≈Çady pyta≈Ñ rekrutacyjnych i co odpowiedzieƒá

**"Dlaczego u≈ºywamy warstw raw ‚Üí staging ‚Üí marts?"**
‚Üí Bo chcemy mieƒá kontrolƒô nad transformacjƒÖ danych, ≈Çatwo debugowaƒá, trzymaƒá surowe kopie dla audytu i mieƒá wydajne tabele do raport√≥w.

**"Co przechowujemy w ka≈ºdej warstwie?"**
- Raw ‚Üí nienaruszone dane z systemu ≈∫r√≥d≈Çowego (JSON, CSV, logi).
- Staging ‚Üí dane oczyszczone, znormalizowane, gotowe do transformacji.
- Marts ‚Üí dane agregowane i denormalizowane, przygotowane do raport√≥w i analizy.

**"Czy raw i staging muszƒÖ byƒá w osobnych schematach?"**
‚Üí Nie, to kwestia organizacyjna. Schematy pomagajƒÖ w separacji i bezpiecze≈Ñstwie, ale mo≈ºna trzymaƒá wszystkie tabele w public.

**"Co mo≈ºesz zrobiƒá w marts, czego nie robisz w raw?"**
‚Üí Agregacje, ranking u≈ºytkownik√≥w (ROW_NUMBER()), sumy zam√≥wie≈Ñ, materializowane widoki, indeksy pod raporty.

**"Jakie problemy rozwiƒÖzujƒÖ mart tables?"**
‚Üí Wydajno≈õƒá zapyta≈Ñ analitycznych, przygotowanie danych do BI, sp√≥jno≈õƒá danych po transformacjach.

### 5Ô∏è‚É£ Dodatkowe tipy do rozmowy

- Warto wspomnieƒá o dbt ‚Äì ≈õwietne narzƒôdzie do warstwowania:
  - Raw ‚Üí source
  - Staging ‚Üí stg_ modele
  - Mart ‚Üí fct_, dim_ modele

- Mo≈ºesz wspomnieƒá o incremental loads: w raw wrzucamy wszystko, w staging i marts tylko nowe dane.

- Materialized views w marts ‚Üí przyspieszajƒÖ raporty i mo≈ºna je od≈õwie≈ºaƒá co np. godzinƒô.

**Output:**
- schema bazy: raw tables, staging, marts (np. w formie pliku .sql lub diagramu)

### Postgres / Redshift basics - Implementacja

**Wymagania:**
- Python 3.8+
- PostgreSQL na AWS RDS (lub lokalnie)
- Dane po≈ÇƒÖczenia do bazy danych

**Kroki:**

1. **Instalacja zale≈ºno≈õci Python:**
```bash
cd scripts/python
pip install -r requirements.txt
```

2. **Konfiguracja po≈ÇƒÖczenia:**
   
   Utw√≥rz plik `.env` w g≈Ç√≥wnym katalogu projektu z danymi po≈ÇƒÖczenia:
```env
POSTGRES_HOST=your-rds-endpoint.xxxxx.eu-central-1.rds.amazonaws.com
POSTGRES_PORT=5432
POSTGRES_DB=postgres
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_secure_password
```

   **Gdzie znale≈∫ƒá dane po≈ÇƒÖczenia RDS:**
   - AWS Console ‚Üí RDS ‚Üí Databases ‚Üí wybierz swojƒÖ bazƒô
   - Endpoint (host): `your-db-name.xxxxx.region.rds.amazonaws.com`
   - Port: domy≈õlnie 5432 dla PostgreSQL
   - Database name: nazwa bazy (np. `postgres`)
   - Master username: u≈ºytkownik g≈Ç√≥wny
   - Password: has≈Ço ustawione przy tworzeniu

3. **Uruchomienie skryptu CRUD:**
```bash
# Z g≈Ç√≥wnego katalogu projektu
python scripts/python/postgres_crud.py
```

**Co robi skrypt:**
- ‚úÖ **CREATE:** Tworzy tabelƒô testowƒÖ `volt_test_users`
- ‚úÖ **INSERT:** Wstawia przyk≈Çadowe dane u≈ºytkownik√≥w
- ‚úÖ **SELECT:** Odczytuje wszystkie rekordy i pokazuje wyniki
- ‚úÖ **UPDATE:** Aktualizuje wybrany rekord
- ‚úÖ **DELETE:** Usuwa przyk≈Çadowy rekord
- ‚úÖ Pokazuje wyniki ka≈ºdej operacji w konsoli

**Struktura skryptu:**
```
scripts/python/
‚îú‚îÄ‚îÄ postgres_crud.py      # G≈Ç√≥wny skrypt z operacjami CRUD
‚îî‚îÄ‚îÄ requirements.txt      # Zale≈ºno≈õci Python (psycopg2-binary, python-dotenv)
```

**Bezpiecze≈Ñstwo:**
- U≈ºyj `.env` dla danych po≈ÇƒÖczenia (nie commituj `.env` do git!)
- `.env` jest ju≈º w `.gitignore`
- Skrypt u≈ºywa parametr√≥wzowanych zapyta≈Ñ (zapobiega SQL injection)

**Rozszerzenia (opcjonalne):**
- Dodaj wiƒôcej operacji (JOIN, GROUP BY, agregacje)
- Po≈ÇƒÖcz z Node.js API (dodaj endpoint do server/routes)
- Stw√≥rz bardziej z≈Ço≈ºone zapytania z CTE i window functions

---

## Tydzie≈Ñ 3: S3 jako Data Lake
Cel: separacja storage vs compute

**Checklist:**
- [ ] **bucket: raw/, staging/, mart/:** Stw√≥rz strukturƒô folder√≥w w S3 (raw, staging, mart) pod r√≥≈ºne etapy przetwarzania danych.
- [ ] **format: CSV ‚Üí Parquet:** Dodaj w projekcie migracjƒô plik√≥w CSV ‚Üí Parquet (np. przez Pandas/Python) ‚Äì przetestuj upload i odczyt Parquet.
- [ ] **S3 lifecycle policies:** Dodaj polityki automatycznego czyszczenia/przenoszenia danych (np. raw po 30 dniach ‚Üí Glacier, staging ‚Üí kasowanie po tygodniu).
- [ ] **partycjonowanie:** Dodaj partycjonowanie po dacie (dt=YYYY-MM-DD) w strukturze folder√≥w.
- [ ] **integracja:** Poka≈º jak dane p≈ÇynƒÖ z S3 do Postgres marts.

**Output:**
- dane lƒÖdujƒÖ w S3, README "data lake layout"
- partycjonowanie zaimplementowane
- lifecycle policies skonfigurowane
- przyk≈Çadowy kod konwersji CSV‚ÜíParquet

---

Zadanie 1

Zbuduj data lake w S3 z podzia≈Çem na raw/staging/mart dla danych u≈ºytkownik√≥w.

Zadanie 2

Dodaj partycjonowanie po dacie i poka≈º, dlaczego to poprawia wydajno≈õƒá.

Zadanie 3

Zaimplementuj lifecycle policy i opisz decyzje kosztowe w README.

## Tydzie≈Ñ 4: Python + S3 Podstawy (Integracja z aplikacjƒÖ Volt)

Zadanie 1

Zbuduj Python pipeline do migracji danych komponent√≥w elektrycznych z PostgreSQL do S3. Stw√≥rz strukturƒô folder√≥w: raw/components/, staging/components/, mart/components/.

Zadanie 2

Zaimplementuj migracjƒô zdjƒôƒá komponent√≥w z lokalnego storage (frontend/public/pictures/electricComponents/) do S3 bucket. Zachowaj nazewnictwo plik√≥w i dodaj metadane.

Zadanie 3

Stw√≥rz system presigned URLs do bezpiecznego dostƒôpu do zdjƒôƒá komponent√≥w w S3. Zintegruj z istniejƒÖcym Node.js API Volt.



**Cel:** Zintegrowanie wszystkiego w dzia≈ÇajƒÖcy end-to-end pipeline.

**Zadania:**
- [ ] **Pe≈Çny pipeline:** Postgres Volt + API ‚Üí Pandas ‚Üí Parquet ‚Üí S3
- [ ] **Lambda trigger:** Automatyczna obr√≥bka upload√≥w
- [ ] **Airflow orchestration:** DAG zarzƒÖdzajƒÖcy ca≈Çym procesem
- [ ] **Monitoring:** Logi, alerty, dashboard statusu

**Mini-projekt Volt:**
```
1. Pobierz nowe komponenty z Postgres Volt
2. Pobierz dodatkowe dane z API (np. ceny rynkowe)
3. Transformacja Pandas (oczyszczanie, agregacje)
4. Konwersja do Parquet
5. Upload do S3 z partycjonowaniem
6. Lambda walidacja i dodatkowe transformacje
7. Za≈Çadunek do analitycznej bazy danych
8. Monitoring i alerty
```

**Output:** Kompletny, produkcyjny pipeline dla aplikacji Volt!

---

## Implementacja - Szczeg√≥≈Çowe instrukcje

### Dzie≈Ñ 1-2: Python + AWS S3 (boto3) - podstawy z Volt

**Cel:** Opanowaƒá boto3 i podstawowe operacje S3 u≈ºywajƒÖc danych z aplikacji Volt.

**Zadania:**
- [ ] **Instalacja boto3:** `pip install boto3`
- [ ] **Konfiguracja AWS:** `aws configure`, IAM role z uprawnieniami S3
- [ ] **Tworzenie bucket√≥w:** Stw√≥rz bucket `volt-data-lake`
- [ ] **Upload/download plik√≥w:** Prze≈õlij przyk≈Çadowe pliki CSV i zdjƒôcia z Volt
- [ ] **Presigned URLs:** Wygeneruj bezpieczne linki do zdjƒôƒá komponent√≥w
- [ ] **ZarzƒÖdzanie uprawnieniami:** Skonfiguruj IAM policies z least privilege

**ƒÜwiczenie praktyczne z Volt:**
```python
# Stw√≥rz bucket ‚Üí wgraj dane komponent√≥w z PostgreSQL ‚Üí pobierz je ‚Üí usu≈Ñ testowe pliki
from scripts.python.week4.volt_data_to_s3 import export_components_to_json, upload_components_to_s3
components = export_components_to_json()
upload_components_to_s3(components)
```

**Output:** Bucket S3 z przyk≈Çadowymi danymi, dzia≈ÇajƒÖce presigned URLs.

---

### Dzie≈Ñ 3-4: Praca z danymi ‚Äì CSV/JSON/Parquet + Volt

**Cel:** Nauczyƒá siƒô transformacji danych u≈ºywajƒÖc rzeczywistych danych z aplikacji Volt.

**Zadania:**
- [ ] **Pandas basics:** `pip install pandas pyarrow`
- [ ] **Wczytywanie danych:** CSV/JSON z PostgreSQL Volt
- [ ] **Transformacje:** Czyszczenie, filtrowanie danych komponent√≥w
- [ ] **Zapis Parquet:** Konwersja danych Volt do formatu Parquet
- [ ] **Upload do S3:** Przesy≈Çanie przetworzonych danych

**ƒÜwiczenie z Volt:**
```python
# Pobierz dane komponent√≥w z Postgres ‚Üí Pandas ‚Üí Parquet ‚Üí S3
import pandas as pd
from scripts.python.week4.volt_data_to_s3 import get_postgres_connection

conn = get_postgres_connection()
df = pd.read_sql("SELECT * FROM electrical_components WHERE price > 0", conn)
df.to_parquet("volt_components.parquet")
# Upload do S3...
```

**Output:** Dane komponent√≥w w formacie Parquet w S3, skrypt transformacji.

---

### Dzie≈Ñ 5: Eventy S3 + Lambda (Python) + Volt

**Cel:** Event-driven processing dla automatycznej obr√≥bki danych z aplikacji Volt.

**Zadania:**
- [ ] **Lambda function:** Stw√≥rz funkcjƒô do konwersji CSV‚ÜíParquet
- [ ] **S3 events:** Skonfiguruj trigger na upload plik√≥w do S3
- [ ] **Testowanie lokalnie:** U≈ºyj AWS SAM lub mock√≥w boto3
- [ ] **Integracja z Volt:** Lambda wywo≈Çuje siƒô przy uploadzie zdjƒôƒá

**ƒÜwiczenie z Volt:**
```python
# Upload zdjƒôcia komponentu ‚Üí Lambda zmniejsza rozmiar ‚Üí zapis do innego bucketu
# U≈ºyj scripts/python/week5/lambda_csv_to_parquet.py jako template
```

**Output:** Lambda function reagujƒÖca na uploady plik√≥w z aplikacji Volt.

---

### Dzie≈Ñ 6-7: Integracja z API / Secrets Manager + Volt

**Cel:** ≈ÅƒÖczenie danych z aplikacji Volt z zewnƒôtrznymi ≈∫r√≥d≈Çami.

**Zadania:**
- [ ] **API Gateway:** REST API do pobierania danych
- [ ] **Secrets Manager:** Bezpieczne przechowywanie kluczy bazy danych
- [ ] **≈ÅƒÖczenie danych:** API + PostgreSQL Volt + S3
- [ ] **Error handling:** Retry, timeout, logging b≈Çƒôd√≥w

**ƒÜwiczenie z Volt:**
```python
# Pobierz dane z zewnƒôtrznego API ‚Üí po≈ÇƒÖcz z danymi komponent√≥w ‚Üí zapisz do S3 ‚Üí log w Postgres
# Integracja z istniejƒÖcym Node.js API Volt
```

**Output:** Python API endpoints komunikujƒÖce siƒô z aplikacjƒÖ Volt.

---

## Tydzie≈Ñ 2: Pipeline'y i Airflow

---

### Tydzie≈Ñ 4: Python + S3 Podstawy - Implementacja

**Wymagania:**
- Python 3.8+
- AWS account z skonfigurowanym S3
- Dane po≈ÇƒÖczenia do PostgreSQL (z tygodnia 2)
- IstniejƒÖca aplikacja Volt (TypeScript/Node.js)

**Konfiguracja AWS:**
1. **Utw√≥rz IAM User/Role** z uprawnieniami do S3:
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject",
        "s3:DeleteObject",
        "s3:ListBucket",
        "s3:GetBucketLocation"
      ],
      "Resource": [
        "arn:aws:s3:::your-volt-bucket/*",
        "arn:aws:s3:::your-volt-bucket"
      ]
    }
  ]
}
```

2. **Skonfiguruj AWS credentials:**
```bash
aws configure
# lub u≈ºyj zmiennych ≈õrodowiskowych
```

3. **Dodaj do .env:**
```env
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key
AWS_DEFAULT_REGION=eu-central-1
S3_BUCKET_NAME=volt-data-lake
```

**Struktura projektu:**
```
scripts/python/week4/
‚îú‚îÄ‚îÄ s3_basics.py           # Podstawowe operacje S3
‚îú‚îÄ‚îÄ volt_data_to_s3.py     # Migracja danych z PostgreSQL do S3
‚îú‚îÄ‚îÄ volt_images_to_s3.py   # Migracja zdjƒôƒá do S3
‚îú‚îÄ‚îÄ presigned_urls.py      # Generator presigned URLs
‚îú‚îÄ‚îÄ requirements.txt       # Dodatkowe zale≈ºno≈õci
‚îî‚îÄ‚îÄ README.md              # Dokumentacja implementacji
```

**Kluczowe skrypty:**

1. **s3_basics.py** - podstawowe operacje:
```python
import boto3
import os
from dotenv import load_load

load_dotenv()

def get_s3_client():
    return boto3.client('s3',
        aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
        aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),
        region_name=os.getenv('AWS_DEFAULT_REGION')
    )

def upload_file_to_s3(file_path, s3_key):
    s3 = get_s3_client()
    bucket = os.getenv('S3_BUCKET_NAME')

    with open(file_path, 'rb') as file:
        s3.put_object(
            Bucket=bucket,
            Key=s3_key,
            Body=file,
            Metadata={
                'uploaded_by': 'volt_app',
                'timestamp': str(datetime.now())
            }
        )
```

2. **volt_data_to_s3.py** - eksport danych z PostgreSQL do S3:
```python
import psycopg2
import json
import pandas as pd
from s3_basics import upload_file_to_s3

def export_components_to_s3():
    # Po≈ÇƒÖcz z PostgreSQL
    conn = psycopg2.connect(
        host=os.getenv('POSTGRES_HOST'),
        database=os.getenv('POSTGRES_DB'),
        user=os.getenv('POSTGRES_USER'),
        password=os.getenv('POSTGRES_PASSWORD')
    )

    # Pobierz dane komponent√≥w
    query = "SELECT * FROM electrical_components"
    df = pd.read_sql_query(query, conn)

    # Zapisz jako JSON do S3
    json_data = df.to_json(orient='records', date_format='iso')
    s3_key = f"raw/components/{datetime.now().strftime('%Y-%m-%d')}/components.json"

    # Upload do S3
    s3 = boto3.client('s3')
    s3.put_object(
        Bucket=os.getenv('S3_BUCKET_NAME'),
        Key=s3_key,
        Body=json_data,
        ContentType='application/json'
    )

    conn.close()
    return s3_key
```

3. **volt_images_to_s3.py** - migracja zdjƒôƒá:
```python
import os
import glob
from s3_basics import upload_file_to_s3

def migrate_images_to_s3():
    # ≈öcie≈ºka do lokalnych zdjƒôƒá
    local_images_path = "../../frontend/public/pictures/electricComponents/*.jpg"

    uploaded_files = []

    for image_path in glob.glob(local_images_path):
        filename = os.path.basename(image_path)
        component_id = filename.split('.')[0]  # nazwa_pliku.jpg ‚Üí nazwa_pliku

        # Struktura w S3: images/components/{component_id}/{filename}
        s3_key = f"images/components/{component_id}/{filename}"

        try:
            upload_file_to_s3(image_path, s3_key)
            uploaded_files.append({
                'component_id': component_id,
                's3_key': s3_key,
                'local_path': image_path
            })
        except Exception as e:
            print(f"Error uploading {filename}: {e}")

    return uploaded_files
```

**Uruchomienie:**
```bash
cd scripts/python/week4
python volt_data_to_s3.py    # Eksport danych do S3
python volt_images_to_s3.py  # Migracja zdjƒôƒá do S3
```

---

### Tydzie≈Ñ 5: Zaawansowane S3 + Pipeline'y - Implementacja

**Wymagania:**
- Wszystko z tygodnia 4
- AWS Lambda, Glue, CloudWatch
- Apache Airflow (lokalnie lub MWAA)

**Konfiguracja event√≥w S3:**

1. **Lambda function** do automatycznej konwersji CSV‚ÜíParquet:
```python
import boto3
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from io import BytesIO

def lambda_handler(event, context):
    s3 = boto3.client('s3')

    # Pobierz informacje o pliku z eventu
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = event['Records'][0]['s3']['object']['key']

    # Sprawd≈∫ czy to CSV w folderze raw
    if not (key.startswith('raw/') and key.endswith('.csv')):
        return

    # Pobierz plik CSV
    response = s3.get_object(Bucket=bucket, Key=key)
    csv_data = pd.read_csv(BytesIO(response['Body'].read()))

    # Konwertuj do Parquet
    buffer = BytesIO()
    table = pa.Table.from_pandas(csv_data)
    pq.write_table(table, buffer)

    # Zapisz w folderze staging jako Parquet
    parquet_key = key.replace('raw/', 'staging/').replace('.csv', '.parquet')
    s3.put_object(
        Bucket=bucket,
        Key=parquet_key,
        Body=buffer.getvalue()
    )

    return {
        'statusCode': 200,
        'body': f'Converted {key} to {parquet_key}'
    }
```

2. **Airflow DAG** dla end-to-end pipeline:
```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import sys
import os

# Dodaj ≈õcie≈ºkƒô do w≈Çasnych skrypt√≥w
sys.path.append('/opt/airflow/scripts')

from week4.volt_data_to_s3 import export_components_to_s3
from week5.glue_etl import run_glue_job

default_args = {
    'owner': 'volt_data_team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'volt_data_pipeline',
    default_args=default_args,
    description='Volt data pipeline: PostgreSQL ‚Üí S3 ‚Üí Glue ‚Üí Redshift',
    schedule_interval='@daily',
    catchup=False
)

extract_to_s3 = PythonOperator(
    task_id='extract_volt_data_to_s3',
    python_callable=export_components_to_s3,
    dag=dag
)

run_etl = PythonOperator(
    task_id='run_glue_etl_job',
    python_callable=run_glue_job,
    dag=dag
)

# Kolejno≈õƒá zada≈Ñ
extract_to_s3 >> run_etl
```

**Konfiguracja Lifecycle Policies:**
```json
{
  "Rules": [
    {
      "ID": "Move raw data to Glacier after 30 days",
      "Status": "Enabled",
      "Prefix": "raw/",
      "Transitions": [
        {
          "Days": 30,
          "StorageClass": "GLACIER"
        }
      ]
    },
    {
      "ID": "Delete staging data after 7 days",
      "Status": "Enabled",
      "Prefix": "staging/",
      "Expiration": {
        "Days": 7
      }
    }
  ]
}
```

**Struktura projektu week5:**
```
scripts/python/week5/
‚îú‚îÄ‚îÄ lambda_csv_to_parquet.py    # Lambda function
‚îú‚îÄ‚îÄ glue_etl_job.py            # Glue ETL job
‚îú‚îÄ‚îÄ airflow_dag.py             # Airflow DAG
‚îú‚îÄ‚îÄ s3_lifecycle.py            # Skrypt do konfiguracji lifecycle
‚îú‚îÄ‚îÄ partitioning_utils.py      # Utilities do partycjonowania
‚îî‚îÄ‚îÄ README.md                  # Szczeg√≥≈Çowa dokumentacja
```

**Uruchomienie:**
```bash
cd scripts/python/week5
python s3_lifecycle.py        # Skonfiguruj lifecycle policies
# Deploy Lambda przez AWS Console lub SAM
# Skonfiguruj Glue job przez AWS Console
# Uruchom Airflow DAG lokalnie lub w MWAA
```

---


### Dzie≈Ñ 8-9: Airflow + S3 + Volt

**Cel:** Orkiestracja pipeline'√≥w danych dla aplikacji Volt.

**Zadania:**
- [ ] **Instalacja Airflow:** `pip install apache-airflow`
- [ ] **Podstawy DAG:** Directed Acyclic Graphs
- [ ] **Operatorzy:** S3Hook, PythonOperator, PostgresOperator
- [ ] **Monitoring:** Logi, alerty, status zada≈Ñ

**ƒÜwiczenie z Volt:**
```python
# DAG: Pobierz nowe komponenty z Postgres ‚Üí transform Pandas ‚Üí upload do S3
# U≈ºyj danych z aplikacji Volt jako przyk≈Çad
```

**Output:** Dzia≈ÇaƒÖcy DAG Airflow przetwarzajƒÖcy dane z Volt.

---

### Dzie≈Ñ 10-11: Redshift/Athena (opcjonalnie Postgres) + Volt

**Cel:** Analiza danych z S3 u≈ºywajƒÖc r√≥≈ºnych narzƒôdzi.

**Zadania:**
- [ ] **Schematy w Postgres:** Tabele analityczne dla danych Volt
- [ ] **COPY z S3:** ≈Åadowanie danych Parquet do Postgres
- [ ] **Query danych:** Agregacje, JOIN z oryginalnymi danymi Volt
- [ ] **Athena:** Query bezpo≈õrednio na S3 (bez kopiowania)

**ƒÜwiczenie z Volt:**
```python
# Z S3 wrzuƒá dane komponent√≥w ‚Üí Postgres ‚Üí zapytanie o top komponenty ‚Üí wynik do S3
# Po≈ÇƒÖcz z istniejƒÖcƒÖ tabelƒÖ electrical_components
```

**Output:** Analityczna baza danych z danymi z aplikacji Volt.

---

### Dzie≈Ñ 12: Transformacje i automatyzacja + Volt

**Cel:** Zaawansowane transformacje danych z aplikacji Volt.

**Zadania:**
- [ ] **Pandas transformacje:** Zaawansowane operacje na danych
- [ ] **Walidacja danych:** Sprawdzenie poprawno≈õci danych Volt
- [ ] **Filtrowanie:** Usuwanie b≈Çƒôdnych/duplikat√≥w komponent√≥w
- [ ] **Testowanie:** Unit tests dla funkcji transformacji

**ƒÜwiczenie z Volt:**
```python
# Walidacja danych komponent√≥w: cena > 0, nazwa nie pusta, prawid≈Çowe typy
# Transformacje: normalizacja nazw, kategorie, obliczone kolumny
```

**Output:** Czysty, przetworzony dataset z aplikacji Volt.

---

### Dzie≈Ñ 13: Monitoring i logowanie + Volt

**Cel:** Monitoring pipeline'√≥w przetwarzajƒÖcych dane z aplikacji Volt.

**Zadania:**
- [ ] **CloudWatch:** Logi dla Lambda i S3 (je≈õli AWS)
- [ ] **Python logging:** Szczeg√≥≈Çowe logi operacji
- [ ] **Airflow monitoring:** Status DAG, alerty b≈Çƒôd√≥w
- [ ] **Metrics:** Czas wykonania, liczba przetworzonych rekord√≥w

**ƒÜwiczenie z Volt:**
```python
# Logowanie ka≈ºdej operacji na danych komponent√≥w
# Alert przy b≈Çƒôdzie w pipeline Volt ‚Üí S3
```

**Output:** Kompletny monitoring pipeline'√≥w Volt.

---

### Dzie≈Ñ 14: Mini-projekt ko≈Ñcowy - Pe≈Çny Pipeline Volt


### Przygotowanie ≈õrodowiska dla wszystkich tygodni

**Wymagania:**
- Python 3.8+
- AWS account z dostƒôpem do S3
- PostgreSQL z danymi aplikacji Volt
- Node.js aplikacja Volt (do testowania integracji)

**Instalacja zale≈ºno≈õci:**
```bash
cd scripts/python
pip install -r requirements.txt
# Dodatkowo dla pe≈Çnego programu:
pip install pandas pyarrow tenacity requests apache-airflow
```

**Konfiguracja AWS:**
```bash
aws configure
# Ustaw region, access key, secret key
```

**Zmienne ≈õrodowiskowe (.env):**
```env
# AWS S3
AWS_ACCESS_KEY_ID=your_key
AWS_SECRET_ACCESS_KEY=your_secret
AWS_DEFAULT_REGION=eu-central-1
S3_BUCKET_NAME=volt-data-lake

# PostgreSQL (z aplikacji Volt)
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=volt_db
POSTGRES_USER=volt_user
POSTGRES_PASSWORD=your_password
```

### Przydatne skrypty startowe:

**Test po≈ÇƒÖczenia z S3:**
```python
from scripts.python.week4.s3_config import s3_config
s3_config.test_connection()
```

**Szybki upload danych z Volt:**
```python
from scripts.python.week4.volt_data_to_s3 import export_components_to_json, upload_components_to_s3
data = export_components_to_json()
upload_components_to_s3(data)
```

**Test presigned URL:**
```python
from scripts.python.week4.presigned_urls import generate_component_image_url
url_data = generate_component_image_url("123", expiration_hours=24)
print(url_data['presigned_url'])
```

### Setup Airflow (Dzie≈Ñ 8-9):

```bash
pip install apache-airflow
airflow db init
airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com
```

**Podstawowy DAG dla Volt:**
```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

from scripts.python.week4.volt_data_to_s3 import export_components_to_json, upload_components_to_s3

def volt_etl():
    data = export_components_to_json()
    return upload_components_to_s3(data)

dag = DAG('volt_data_pipeline', start_date=datetime(2024, 1, 1), schedule_interval='@daily')

etl_task = PythonOperator(
    task_id='extract_transform_load',
    python_callable=volt_etl,
    dag=dag
)
```

---

**Wskaz√≥wka:** Ka≈ºdego dnia oznaczaj wykonane zadania ‚úì i zapisuj notatki z problemami/rozwiƒÖzaniami. Na koniec dnia zr√≥b kr√≥tkie podsumowanie tego, czego siƒô nauczy≈Çe≈õ. To bƒôdzie ≈õwietna dokumentacja Twojej nauki!
