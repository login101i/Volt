# Data Engineer Roadmap Volt â€“ praktyka na podstawie roadmapy (Node.js, Next.js API, Python)

## Cel projektu
Volt ewoluuje w realny projekt data engineeringowy â€“ nie tylko teoria, ale wdroÅ¼enia hands-on! Stack: Node.js/Next.js API + Python + AWS.

---

## TydzieÅ„ 1: AWS & Architektura
Cel: rozumiesz, co gdzie stoi i dlaczego 

**Checklist:**
- [x] **VPC:** Izoluj zasoby AWS wÅ‚asnÄ…, prywatnÄ… sieciÄ… (stwÃ³rz VPC przez konsolÄ™, Terraform lub AWS CLI). 
- [x] **public / private subnet:** UtwÃ³rz subnety i poÅ‚Ä…cz je z VPC â€“ backend w prywatnych, LB/API w publicznych (high-availability!).
- [x] **security groups:** Ustaw reguÅ‚y dostÄ™pu: tylko niezbÄ™dne porty, backend/API â€“ tylko swoje grupy/security references! 
- [x] **EC2 public + private:** Postaw EC2 (public jako bastion, prywatne pod backend/Python/ETL). 
- [x] **RDS (Postgres):** Postaw bazÄ™ (tylko w prywatnym subnetcie!) i poÅ‚Ä…cz z Next.js API lub Python workerem, nie otwieraj do Å›wiata. 
- [x] **Route53:** ObsÅ‚uÅ¼ routing domen (np. pod API, LB, backend, dev/prod Å›rodowiska). Testuj rÃ³Å¼ne typy rekordÃ³w. 
- [x] **Reverse proxy (Caddy/Nginx):** ZarzÄ…dzaj SSL, rozdziel ruchem do usÅ‚ug (np. /api â†’ backend, /airflow â†’ Airflow UI) na jednej maszynie/instancji. 

**Output:**
- diagram architektury (np. draw.io)
- README czemu tak (decyzje architektoniczne)

---

## TydzieÅ„ 2: SQL & Model Danych
Cel: myÅ›lisz jak Data Engineer, nie jak "SELECT *"

**Checklist:**
- [x] **Postgres / Redshift basics:** Postaw bazÄ™, skonfiguruj poÅ‚Ä…czenie z aplikacjÄ…, wykonaj inspekcjÄ™ bazowych operacji CRUD przez Node/Python.
  - âœ… Utworzono skrypt Python: `scripts/python/postgres_crud.py`
  - âœ… Zobacz instrukcje poniÅ¼ej w sekcji "Postgres / Redshift basics - Implementacja"
- [ ] **CTE:** NapisaÄ‡ kilka zapytaÅ„ z CTE (`WITH`), oswoiÄ‡ siÄ™ z refaktoryzacjÄ… zagnieÅ¼dÅ¼onych SELECTÃ³w.
- [ ] **window functions:** ZrÃ³b query z oknem (np. ranking, sumy ruchome) â€“ przydatne w raportach!
- [ ] **indexing (teoria + praktyka):** Dodaj indeksy do tabel (po kluczach), sprawdÅº wydajnoÅ›Ä‡.
- [ ] **schema bazy: raw tables, staging, marts:** Zaprojektuj model warstwowy (surowe/staging/analityczne tabele); moÅ¼esz podzieliÄ‡ schemat na pliki SQL/dbt.

## ğŸ¯ Podsumowanie: Architektura Warstwowa Danych (Raw â†’ Staging â†’ Marts)

### Dlaczego w ogÃ³le tworzymy warstwy?

**Cel: podziaÅ‚ danych na rÃ³Å¼ne poziomy "dojrzaÅ‚oÅ›ci" i kontroli jakoÅ›ci.**

- **Raw (surowe)** â€“ peÅ‚ne kopie danych z systemu ÅºrÃ³dÅ‚owego, bez zmian.
  - Audyt, odzyskiwanie, powtarzalnoÅ›Ä‡ ETL.
  - MoÅ¼na trzymaÄ‡ w dowolnym formacie (JSONB, CSV, logi).

- **Staging (przygotowane)** â€“ dane juÅ¼ oczyszczone i ustrukturyzowane.
  - Casty typÃ³w, brak nulli w kluczowych kolumnach, standaryzacja.
  - Nadal blisko ÅºrÃ³dÅ‚a, ale juÅ¼ moÅ¼na robiÄ‡ transformacje.

- **Msarts (analityczne / raportowe)** â€“ dane gotowe do raportowania i analizy.
  - Agregacje, denormalizacja, obliczone kolumny (np. total_orders w naszym users_mart).
  - Tu zakÅ‚ada siÄ™ wysokÄ… wydajnoÅ›Ä‡ zapytaÅ„ â€“ indeksy, materializacje, partycje.

**PodsumowujÄ…c:** kaÅ¼da warstwa daje kontrolÄ™ nad transformacjÄ… danych i uÅ‚atwia debugowanie: jeÅ›li raport jest bÅ‚Ä™dny, moÅ¼esz sprawdziÄ‡ raw â†’ staging â†’ mart krok po kroku.

### 2ï¸âƒ£ Co moÅ¼emy robiÄ‡ na kaÅ¼dej warstwie

| Warstwa | Co robimy | PrzykÅ‚ady |
|---------|-----------|-----------|
| **Raw** | Archiwizacja, audyt | Trzymanie JSONÃ³w z API, peÅ‚nych dumpÃ³w, logÃ³w. |
| **Staging** | Czyszczenie, typy, walidacja | CAST JSON â†’ kolumny, usuniÄ™cie duplikatÃ³w, normalizacja nazw. |
| **Marts** | Analiza, raporty, agregacje | Liczenie sum, ranking uÅ¼ytkownikÃ³w, ostatnie zamÃ³wienie, liczba zamÃ³wieÅ„, widoki do BI. |

### 3ï¸âƒ£ KorzyÅ›ci z takiego podejÅ›cia

- **BezpieczeÅ„stwo i audyt** â€“ surowe dane pozostajÄ… nienaruszone.
- **ÅatwoÅ›Ä‡ debugowania** â€“ jeÅ›li coÅ› jest Åºle w raporcie, sprawdzasz krok po kroku: raw â†’ staging â†’ mart.
- **SkalowalnoÅ›Ä‡** â€“ w miarÄ™ rozrostu danych, moÅ¼esz przetwarzaÄ‡ tylko staging/marts zamiast od nowa caÅ‚ych danych.
- **Przygotowanie pod ETL / ELT / BI** â€“ warstwy idealnie wspÃ³Å‚grajÄ… z narzÄ™dziami typu dbt, Airflow, Looker, Power BI.
- **WydajnoÅ›Ä‡** â€“ w marts moÅ¼esz dodawaÄ‡ indeksy, partycje, materializowane widoki, Å¼eby raporty dziaÅ‚aÅ‚y szybko.

### 4ï¸âƒ£ PrzykÅ‚ady pytaÅ„ rekrutacyjnych i co odpowiedzieÄ‡

**"Dlaczego uÅ¼ywamy warstw raw â†’ staging â†’ marts?"**
â†’ Bo chcemy mieÄ‡ kontrolÄ™ nad transformacjÄ… danych, Å‚atwo debugowaÄ‡, trzymaÄ‡ surowe kopie dla audytu i mieÄ‡ wydajne tabele do raportÃ³w.

**"Co przechowujemy w kaÅ¼dej warstwie?"**
- Raw â†’ nienaruszone dane z systemu ÅºrÃ³dÅ‚owego (JSON, CSV, logi).
- Staging â†’ dane oczyszczone, znormalizowane, gotowe do transformacji.
- Marts â†’ dane agregowane i denormalizowane, przygotowane do raportÃ³w i analizy.

**"Czy raw i staging muszÄ… byÄ‡ w osobnych schematach?"**
â†’ Nie, to kwestia organizacyjna. Schematy pomagajÄ… w separacji i bezpieczeÅ„stwie, ale moÅ¼na trzymaÄ‡ wszystkie tabele w public.

**"Co moÅ¼esz zrobiÄ‡ w marts, czego nie robisz w raw?"**
â†’ Agregacje, ranking uÅ¼ytkownikÃ³w (ROW_NUMBER()), sumy zamÃ³wieÅ„, materializowane widoki, indeksy pod raporty.

**"Jakie problemy rozwiÄ…zujÄ… mart tables?"**
â†’ WydajnoÅ›Ä‡ zapytaÅ„ analitycznych, przygotowanie danych do BI, spÃ³jnoÅ›Ä‡ danych po transformacjach.

### 5ï¸âƒ£ Dodatkowe tipy do rozmowy

- Warto wspomnieÄ‡ o dbt â€“ Å›wietne narzÄ™dzie do warstwowania:
  - Raw â†’ source
  - Staging â†’ stg_ modele
  - Mart â†’ fct_, dim_ modele

- MoÅ¼esz wspomnieÄ‡ o incremental loads: w raw wrzucamy wszystko, w staging i marts tylko nowe dane.

- Materialized views w marts â†’ przyspieszajÄ… raporty i moÅ¼na je odÅ›wieÅ¼aÄ‡ co np. godzinÄ™.

**Output:**
- schema bazy: raw tables, staging, marts (np. w formie pliku .sql lub diagramu)

### Postgres / Redshift basics - Implementacja

**Wymagania:**
- Python 3.8+
- PostgreSQL na AWS RDS (lub lokalnie)
- Dane poÅ‚Ä…czenia do bazy danych

**Kroki:**

1. **Instalacja zaleÅ¼noÅ›ci Python:**
```bash
cd scripts/python
pip install -r requirements.txt
```

2. **Konfiguracja poÅ‚Ä…czenia:**
   
   UtwÃ³rz plik `.env` w gÅ‚Ã³wnym katalogu projektu z danymi poÅ‚Ä…czenia:
```env
POSTGRES_HOST=your-rds-endpoint.xxxxx.eu-central-1.rds.amazonaws.com
POSTGRES_PORT=5432
POSTGRES_DB=postgres
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_secure_password
```

   **Gdzie znaleÅºÄ‡ dane poÅ‚Ä…czenia RDS:**
   - AWS Console â†’ RDS â†’ Databases â†’ wybierz swojÄ… bazÄ™
   - Endpoint (host): `your-db-name.xxxxx.region.rds.amazonaws.com`
   - Port: domyÅ›lnie 5432 dla PostgreSQL
   - Database name: nazwa bazy (np. `postgres`)
   - Master username: uÅ¼ytkownik gÅ‚Ã³wny
   - Password: hasÅ‚o ustawione przy tworzeniu

3. **Uruchomienie skryptu CRUD:**
```bash
# Z gÅ‚Ã³wnego katalogu projektu
python scripts/python/postgres_crud.py
```

**Co robi skrypt:**
- âœ… **CREATE:** Tworzy tabelÄ™ testowÄ… `volt_test_users`
- âœ… **INSERT:** Wstawia przykÅ‚adowe dane uÅ¼ytkownikÃ³w
- âœ… **SELECT:** Odczytuje wszystkie rekordy i pokazuje wyniki
- âœ… **UPDATE:** Aktualizuje wybrany rekord
- âœ… **DELETE:** Usuwa przykÅ‚adowy rekord
- âœ… Pokazuje wyniki kaÅ¼dej operacji w konsoli

**Struktura skryptu:**
```
scripts/python/
â”œâ”€â”€ postgres_crud.py      # GÅ‚Ã³wny skrypt z operacjami CRUD
â””â”€â”€ requirements.txt      # ZaleÅ¼noÅ›ci Python (psycopg2-binary, python-dotenv)
```

**BezpieczeÅ„stwo:**
- UÅ¼yj `.env` dla danych poÅ‚Ä…czenia (nie commituj `.env` do git!)
- `.env` jest juÅ¼ w `.gitignore`
- Skrypt uÅ¼ywa parametrÃ³wzowanych zapytaÅ„ (zapobiega SQL injection)

**Rozszerzenia (opcjonalne):**
- Dodaj wiÄ™cej operacji (JOIN, GROUP BY, agregacje)
- PoÅ‚Ä…cz z Node.js API (dodaj endpoint do server/routes)
- StwÃ³rz bardziej zÅ‚oÅ¼one zapytania z CTE i window functions

---

## TydzieÅ„ 3: S3 jako Data Lake
Cel: separacja storage vs compute

**Checklist:**
- [ ] **bucket: raw/, staging/, mart/:** StwÃ³rz strukturÄ™ folderÃ³w w S3 (raw, staging, mart) pod rÃ³Å¼ne etapy przetwarzania danych.
- [ ] **format: CSV â†’ Parquet:** Dodaj w projekcie migracjÄ™ plikÃ³w CSV â†’ Parquet (np. przez Pandas/Python) â€“ przetestuj upload i odczyt Parquet.
- [ ] **S3 lifecycle policies:** Dodaj polityki automatycznego czyszczenia/przenoszenia danych (np. raw po 30 dniach â†’ Glacier, staging â†’ kasowanie po tygodniu).
- [ ] **partycjonowanie:** Dodaj partycjonowanie po dacie (dt=YYYY-MM-DD) w strukturze folderÃ³w.
- [ ] **integracja:** PokaÅ¼ jak dane pÅ‚ynÄ… z S3 do Postgres marts.

**Output:**
- dane lÄ…dujÄ… w S3, README "data lake layout"
- partycjonowanie zaimplementowane
- lifecycle policies skonfigurowane
- przykÅ‚adowy kod konwersji CSVâ†’Parquet

---

Zadanie 1

Zbuduj data lake w S3 z podziaÅ‚em na raw/staging/mart dla danych uÅ¼ytkownikÃ³w.

Zadanie 2

Dodaj partycjonowanie po dacie i pokaÅ¼, dlaczego to poprawia wydajnoÅ›Ä‡.

Zadanie 3

Zaimplementuj lifecycle policy i opisz decyzje kosztowe w README.

## TydzieÅ„ 4: Python ETL (bez Airflow)
Cel: czysty, testowalny ETL

**Checklist:**
- [ ] **Python: requests:** Pobranie danych z wybranego API do Pythona.
- [ ] **retry:** Zaimplementuj retry (np. z bibliotekÄ… `tenacity`), niech ETL radzi sobie z awariami sieci.
- [ ] **pagination:** ObsÅ‚uga API ze stronicowaniem.
- [ ] **auth:** ObsÅ‚uga API z autoryzacjÄ…/tokenami.
- [ ] **rate limits:** Zaimplementuj sleep/throttle na api i loguj nadmiarowe wywoÅ‚ania.
- [ ] **ETL: API â†’ S3:** Zbuduj mini-task, ktÃ³ry pobierze dane i zapisze je jako plik do S3 (moÅ¼esz uÅ¼yÄ‡ boto3 lub SDK AWS).
- [ ] **logi:** KaÅ¼dy krok pipeline powinien mieÄ‡ logi (console/log file).
- [ ] **obsÅ‚uga bÅ‚Ä™dÃ³w:** ETL musi obsÅ‚uÅ¼yÄ‡ bÅ‚Ä™dy z API/sieci â€“ testuj! 

**Output:**
- repo folder `etl/`, README + diagram przepÅ‚ywu danych

---

**WskazÃ³wka:** utrzymuj checklistÄ™ i podsumowania tydzieÅ„ po tygodniu; kopiuj praktyczne notatki do wybranego folderu/projektu. KaÅ¼de zadanie odhaczane = realny progres z praktycznej roadmapy!
